import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
import re # Import the regular expression module

# --- Configuration ---
FILE_PATH = r"C:\Users\acer\Downloads\Enterprise_Sustainable Power Evaluation_Dataset.csv"
SEQUENCE_LENGTH = 10  # Number of historical time steps used to predict the next step

# Define the target variable (Emissions Intensity is a key EPM metric)
TARGET_COLUMN = 'Emissions Intensity (kg CO₂ per MWh)'
# Features to use for prediction \(other key EPM metrics)
FEATURE_COLUMNS = [
    'Revenue (USD)',
    'Net Profit Margin (%)',
    'Energy Efficiency (%)',
    'Renewable Energy Share (%)',
    'Sustainability Score',
    'Innovation Index'
]

def clean_column_name(col_name):
    """Helper function to clean column names using regex substitution."""
    # 1. Replace non-alphanumeric characters (except space) with '_'
    cleaned = re.sub(r'[^A-Za-z0-9%]+', '_', col_name)
    # 2. Strip leading/trailing underscores
    cleaned = cleaned.strip('_')
    # 3. Remove parentheses and percentage signs
    cleaned = re.sub(r'[%()]', '', cleaned)
    return cleaned.replace(' ', '_')

def load_and_preprocess_data(file_path):
    """Loads, cleans, and structures the cross-sectional data into a time series."""
    print("--- 1. Loading and Cleaning Data ---")
    try:
        df = pd.read_csv(file_path)
    except FileNotFoundError:
        print(f"Error: File not found at {file_path}. Please ensure the file is in the correct directory.")
        return None, None

    # Standardize column names in the DataFrame
    df.columns = df.columns.str.replace('[^A-Za-z0-9%]+', '_', regex=True).str.strip('_')
    df.columns = df.columns.str.replace('[%()]', '', regex=True)
    df.columns = df.columns.str.replace(' ', '_')

    # Convert the dataset into a single pseudo-time-series
    # We sort by Company_ID just to ensure the sequence is stable across runs,
    # treating each row as a sequential time step for our "Composite Enterprise".
    df_ts = df.sort_values(by='Company_ID').reset_index(drop=True)

    # Apply the custom clean_column_name function to the configuration list
    cleaned_feature_cols = [clean_column_name(col) for col in FEATURE_COLUMNS]
    cleaned_target_col = clean_column_name(TARGET_COLUMN)
    
    cols_to_use = cleaned_feature_cols + [cleaned_target_col]

    # Select only the features and the target, and drop any rows with NaN values
    # Note: We must use the cleaned DataFrame columns here
    df_ts = df_ts[cols_to_use].dropna()

    print(f"Dataset size after cleaning: {df_ts.shape}")
    return df_ts, df_ts.columns

def create_sequences(data, sequence_length):
    """
    Creates sequences of features (X) and the corresponding target value (Y)
    for use in an LSTM model.
    X: [t-N, t-N+1, ..., t-1] (Sequence of historical features)
    Y: [t] (Target value at the next time step)
    """
    X, y = [], []
    for i in range(len(data) - sequence_length):
        # Extract features (all columns except the last one, which is the target)
        X.append(data[i:i + sequence_length, :-1])
        # Extract the target value at the next step (i + sequence_length)
        # Note: We are predicting the target for the step immediately *after* the sequence ends.
        y.append(data[i + sequence_length, -1])
    return np.array(X), np.array(y)

def build_and_train_baseline(df_ts, feature_names):
    """Scales data, creates sequences, and trains the LSTM baseline model."""
    print("\n--- 2. Data Scaling and Sequence Creation ---")

    # The last column is the target (Emissions_Intensity_kg_CO2_per_MWh)
    data = df_ts.values
    
    # Initialize separate scalers for features and target
    scaler_features = MinMaxScaler()
    scaler_target = MinMaxScaler()

    # Scale the features (all columns except the last one)
    features_scaled = scaler_features.fit_transform(data[:, :-1])
    
    # Scale the target separately (the last column)
    target_scaled = scaler_target.fit_transform(data[:, -1].reshape(-1, 1))

    # Recombine scaled data for sequence creation
    scaled_data = np.hstack((features_scaled, target_scaled))
    
    X, y = create_sequences(scaled_data, SEQUENCE_LENGTH)
    
    # Split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, shuffle=False, random_state=42
    )

    print(f"X_train shape: {X_train.shape} (Samples, Time Steps, Features)")
    print(f"y_train shape: {y_train.shape}")
    
    # --- 3. Build LSTM Model (Baseline) ---
    print("\n--- 3. Building and Training LSTM Baseline ---")
    
    # Define the required input shape for the LSTM layers
    input_seq_shape = (X_train.shape[1], X_train.shape[2])

    # Using tf.keras.Input as the first layer to explicitly define shape, 
    # which resolves the UserWarning when defining Sequential models.
    model = Sequential([
        tf.keras.Input(shape=input_seq_shape),
        LSTM(units=50, return_sequences=True),
        Dropout(0.2),
        LSTM(units=50, return_sequences=False),
        Dropout(0.2),
        Dense(units=1)  # Output layer for the single target variable
    ], name="EPM_LSTM_Baseline")
    
    model.compile(optimizer='adam', loss='mse')
    model.summary()
    
    # Train the model
    history = model.fit(
        X_train, y_train,
        epochs=50,
        batch_size=32,
        validation_split=0.1,
        verbose=1,
        shuffle=False # Important for time-series data
    )
    
    # --- 4. Evaluate and Inverse Transform ---
    print("\n--- 4. Evaluating Baseline Performance ---")
    
    # Predict on the test set
    y_pred_scaled = model.predict(X_test)
    
    # Inverse transform to get the prediction in the original scale (kg CO2 per MWh)
    y_test_original = scaler_target.inverse_transform(y_test.reshape(-1, 1))
    y_pred_original = scaler_target.inverse_transform(y_pred_scaled)
    
    # Calculate Mean Absolute Error (MAE) - a common time-series metric
    mae = np.mean(np.abs(y_pred_original - y_test_original))
    
    print(f"\nBaseline Model Evaluation:")
    print(f"Target Variable: {TARGET_COLUMN}")
    print(f"Sequence Length (Lookback): {SEQUENCE_LENGTH} steps")
    print(f"Test MAE (Emissions Intensity): {mae:.2f} kg CO₂ per MWh")
    print(f"This MAE value serves as the benchmark for the Hybrid Model (Phase 3).")


if __name__ == "__main__":
    df_ts, feature_names = load_and_preprocess_data(FILE_PATH)
    if df_ts is not None:
        build_and_train_baseline(df_ts, feature_names)
