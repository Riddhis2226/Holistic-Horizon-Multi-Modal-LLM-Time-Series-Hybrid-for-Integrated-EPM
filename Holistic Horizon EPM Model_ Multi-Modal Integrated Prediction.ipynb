{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25fad4fe-57e5-4e9f-a44d-43d2bb247ecb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Loading and Cleaning Data ---\n",
      "Dataset size after cleaning: (1000, 7)\n",
      "\n",
      "--- 2. Data Scaling and Sequence Creation ---\n",
      "X_train shape: (792, 10, 6) (Samples, Time Steps, Features)\n",
      "y_train shape: (792,)\n",
      "\n",
      "--- 3. Building and Training LSTM Baseline ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"EPM_LSTM_Baseline\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"EPM_LSTM_Baseline\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">11,400</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m50\u001b[0m)              │          \u001b[38;5;34m11,400\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m50\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │          \u001b[38;5;34m20,200\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m51\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,651</span> (123.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m31,651\u001b[0m (123.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,651</span> (123.64 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m31,651\u001b[0m (123.64 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 62ms/step - loss: 0.1360 - val_loss: 0.0877\n",
      "Epoch 2/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0901 - val_loss: 0.0776\n",
      "Epoch 3/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - loss: 0.0861 - val_loss: 0.0778\n",
      "Epoch 4/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.0886 - val_loss: 0.0775\n",
      "Epoch 5/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 49ms/step - loss: 0.0866 - val_loss: 0.0774\n",
      "Epoch 6/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0869 - val_loss: 0.0773\n",
      "Epoch 7/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0855 - val_loss: 0.0773\n",
      "Epoch 8/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0861 - val_loss: 0.0773\n",
      "Epoch 9/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0862 - val_loss: 0.0773\n",
      "Epoch 10/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 0.0873 - val_loss: 0.0774\n",
      "Epoch 11/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0858 - val_loss: 0.0772\n",
      "Epoch 12/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0859 - val_loss: 0.0776\n",
      "Epoch 13/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - loss: 0.0862 - val_loss: 0.0781\n",
      "Epoch 14/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 22ms/step - loss: 0.0856 - val_loss: 0.0774\n",
      "Epoch 15/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - loss: 0.0858 - val_loss: 0.0773\n",
      "Epoch 16/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0872 - val_loss: 0.0772\n",
      "Epoch 17/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0860 - val_loss: 0.0771\n",
      "Epoch 18/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0852 - val_loss: 0.0771\n",
      "Epoch 19/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0840 - val_loss: 0.0771\n",
      "Epoch 20/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0842 - val_loss: 0.0770\n",
      "Epoch 21/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0878 - val_loss: 0.0770\n",
      "Epoch 22/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0858 - val_loss: 0.0771\n",
      "Epoch 23/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0856 - val_loss: 0.0771\n",
      "Epoch 24/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0843 - val_loss: 0.0770\n",
      "Epoch 25/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0850 - val_loss: 0.0770\n",
      "Epoch 26/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0854 - val_loss: 0.0770\n",
      "Epoch 27/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0844 - val_loss: 0.0770\n",
      "Epoch 28/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0854 - val_loss: 0.0771\n",
      "Epoch 29/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0851 - val_loss: 0.0771\n",
      "Epoch 30/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0845 - val_loss: 0.0772\n",
      "Epoch 31/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0832 - val_loss: 0.0771\n",
      "Epoch 32/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0856 - val_loss: 0.0772\n",
      "Epoch 33/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 0.0848 - val_loss: 0.0772\n",
      "Epoch 34/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.0855 - val_loss: 0.0771\n",
      "Epoch 35/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0839 - val_loss: 0.0774\n",
      "Epoch 36/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0844 - val_loss: 0.0774\n",
      "Epoch 37/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.0850 - val_loss: 0.0773\n",
      "Epoch 38/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.0848 - val_loss: 0.0771\n",
      "Epoch 39/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0849 - val_loss: 0.0774\n",
      "Epoch 40/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0836 - val_loss: 0.0772\n",
      "Epoch 41/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0836 - val_loss: 0.0775\n",
      "Epoch 42/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0850 - val_loss: 0.0773\n",
      "Epoch 43/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0847 - val_loss: 0.0774\n",
      "Epoch 44/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.0833 - val_loss: 0.0773\n",
      "Epoch 45/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 0.0830 - val_loss: 0.0775\n",
      "Epoch 46/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 0.0841 - val_loss: 0.0775\n",
      "Epoch 47/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - loss: 0.0836 - val_loss: 0.0775\n",
      "Epoch 48/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0836 - val_loss: 0.0774\n",
      "Epoch 49/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0855 - val_loss: 0.0774\n",
      "Epoch 50/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.0835 - val_loss: 0.0778\n",
      "\n",
      "--- 4. Evaluating Baseline Performance ---\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 76ms/step\n",
      "\n",
      "Baseline Model Evaluation:\n",
      "Target Variable: Emissions Intensity (kg CO₂ per MWh)\n",
      "Sequence Length (Lookback): 10 steps\n",
      "Test MAE (Emissions Intensity): 181.08 kg CO₂ per MWh\n",
      "This MAE value serves as the benchmark for the Hybrid Model (Phase 3).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "import re # Import the regular expression module\n",
    "\n",
    "# --- Configuration ---\n",
    "FILE_PATH = r\"C:\\Users\\acer\\Downloads\\Enterprise_Sustainable Power Evaluation_Dataset.csv\"\n",
    "SEQUENCE_LENGTH = 10  # Number of historical time steps used to predict the next step\n",
    "\n",
    "# Define the target variable (Emissions Intensity is a key EPM metric)\n",
    "TARGET_COLUMN = 'Emissions Intensity (kg CO₂ per MWh)'\n",
    "# Features to use for prediction (other key EPM metrics)\n",
    "FEATURE_COLUMNS = [\n",
    "    'Revenue (USD)',\n",
    "    'Net Profit Margin (%)',\n",
    "    'Energy Efficiency (%)',\n",
    "    'Renewable Energy Share (%)',\n",
    "    'Sustainability Score',\n",
    "    'Innovation Index'\n",
    "]\n",
    "\n",
    "def clean_column_name(col_name):\n",
    "    \"\"\"Helper function to clean column names using regex substitution.\"\"\"\n",
    "    # 1. Replace non-alphanumeric characters (except space) with '_'\n",
    "    cleaned = re.sub(r'[^A-Za-z0-9%]+', '_', col_name)\n",
    "    # 2. Strip leading/trailing underscores\n",
    "    cleaned = cleaned.strip('_')\n",
    "    # 3. Remove parentheses and percentage signs\n",
    "    cleaned = re.sub(r'[%()]', '', cleaned)\n",
    "    return cleaned.replace(' ', '_')\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"Loads, cleans, and structures the cross-sectional data into a time series.\"\"\"\n",
    "    print(\"--- 1. Loading and Cleaning Data ---\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}. Please ensure the file is in the correct directory.\")\n",
    "        return None, None\n",
    "\n",
    "    # Standardize column names in the DataFrame\n",
    "    df.columns = df.columns.str.replace('[^A-Za-z0-9%]+', '_', regex=True).str.strip('_')\n",
    "    df.columns = df.columns.str.replace('[%()]', '', regex=True)\n",
    "    df.columns = df.columns.str.replace(' ', '_')\n",
    "\n",
    "    # Convert the dataset into a single pseudo-time-series\n",
    "    # We sort by Company_ID just to ensure the sequence is stable across runs,\n",
    "    # treating each row as a sequential time step for our \"Composite Enterprise\".\n",
    "    df_ts = df.sort_values(by='Company_ID').reset_index(drop=True)\n",
    "\n",
    "    # Apply the custom clean_column_name function to the configuration list\n",
    "    cleaned_feature_cols = [clean_column_name(col) for col in FEATURE_COLUMNS]\n",
    "    cleaned_target_col = clean_column_name(TARGET_COLUMN)\n",
    "    \n",
    "    cols_to_use = cleaned_feature_cols + [cleaned_target_col]\n",
    "\n",
    "    # Select only the features and the target, and drop any rows with NaN values\n",
    "    # Note: We must use the cleaned DataFrame columns here\n",
    "    df_ts = df_ts[cols_to_use].dropna()\n",
    "\n",
    "    print(f\"Dataset size after cleaning: {df_ts.shape}\")\n",
    "    return df_ts, df_ts.columns\n",
    "\n",
    "def create_sequences(data, sequence_length):\n",
    "    \"\"\"\n",
    "    Creates sequences of features (X) and the corresponding target value (Y)\n",
    "    for use in an LSTM model.\n",
    "    X: [t-N, t-N+1, ..., t-1] (Sequence of historical features)\n",
    "    Y: [t] (Target value at the next time step)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        # Extract features (all columns except the last one, which is the target)\n",
    "        X.append(data[i:i + sequence_length, :-1])\n",
    "        # Extract the target value at the next step (i + sequence_length)\n",
    "        # Note: We are predicting the target for the step immediately *after* the sequence ends.\n",
    "        y.append(data[i + sequence_length, -1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def build_and_train_baseline(df_ts, feature_names):\n",
    "    \"\"\"Scales data, creates sequences, and trains the LSTM baseline model.\"\"\"\n",
    "    print(\"\\n--- 2. Data Scaling and Sequence Creation ---\")\n",
    "\n",
    "    # The last column is the target (Emissions_Intensity_kg_CO2_per_MWh)\n",
    "    data = df_ts.values\n",
    "    \n",
    "    # Initialize separate scalers for features and target\n",
    "    scaler_features = MinMaxScaler()\n",
    "    scaler_target = MinMaxScaler()\n",
    "\n",
    "    # Scale the features (all columns except the last one)\n",
    "    features_scaled = scaler_features.fit_transform(data[:, :-1])\n",
    "    \n",
    "    # Scale the target separately (the last column)\n",
    "    target_scaled = scaler_target.fit_transform(data[:, -1].reshape(-1, 1))\n",
    "\n",
    "    # Recombine scaled data for sequence creation\n",
    "    scaled_data = np.hstack((features_scaled, target_scaled))\n",
    "    \n",
    "    X, y = create_sequences(scaled_data, SEQUENCE_LENGTH)\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, shuffle=False, random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"X_train shape: {X_train.shape} (Samples, Time Steps, Features)\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    \n",
    "    # --- 3. Build LSTM Model (Baseline) ---\n",
    "    print(\"\\n--- 3. Building and Training LSTM Baseline ---\")\n",
    "    \n",
    "    # Define the required input shape for the LSTM layers\n",
    "    input_seq_shape = (X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "    # Using tf.keras.Input as the first layer to explicitly define shape, \n",
    "    # which resolves the UserWarning when defining Sequential models.\n",
    "    model = Sequential([\n",
    "        tf.keras.Input(shape=input_seq_shape),\n",
    "        LSTM(units=50, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(units=50, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(units=1)  # Output layer for the single target variable\n",
    "    ], name=\"EPM_LSTM_Baseline\")\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.summary()\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.1,\n",
    "        verbose=1,\n",
    "        shuffle=False # Important for time-series data\n",
    "    )\n",
    "    \n",
    "    # --- 4. Evaluate and Inverse Transform ---\n",
    "    print(\"\\n--- 4. Evaluating Baseline Performance ---\")\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred_scaled = model.predict(X_test)\n",
    "    \n",
    "    # Inverse transform to get the prediction in the original scale (kg CO2 per MWh)\n",
    "    y_test_original = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "    y_pred_original = scaler_target.inverse_transform(y_pred_scaled)\n",
    "    \n",
    "    # Calculate Mean Absolute Error (MAE) - a common time-series metric\n",
    "    mae = np.mean(np.abs(y_pred_original - y_test_original))\n",
    "    \n",
    "    print(f\"\\nBaseline Model Evaluation:\")\n",
    "    print(f\"Target Variable: {TARGET_COLUMN}\")\n",
    "    print(f\"Sequence Length (Lookback): {SEQUENCE_LENGTH} steps\")\n",
    "    print(f\"Test MAE (Emissions Intensity): {mae:.2f} kg CO₂ per MWh\")\n",
    "    print(f\"This MAE value serves as the benchmark for the Hybrid Model (Phase 3).\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df_ts, feature_names = load_and_preprocess_data(FILE_PATH)\n",
    "    if df_ts is not None:\n",
    "        build_and_train_baseline(df_ts, feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d64af3f4-4f96-43bc-bbe2-373039f4cf38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Loading and Cleaning Data ---\n",
      "Dataset size after cleaning: (1000, 8)\n",
      "--- 2. Simulating Narrative Context and Embeddings ---\n",
      "Simulating 1000 embeddings of dimension 384.\n",
      "\n",
      "--- 3. Data Augmentation Complete ---\n",
      "Saved augmented dataset to: epm_augmented_data_with_embeddings.csv\n",
      "Final shape: (1000, 393) (Includes 384 embedding columns)\n",
      "This file is now ready for the Hybrid Model (Phase 3).\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import re\n",
    "import random\n",
    "\n",
    "# --- Configuration (matching Phase 1) ---\n",
    "FILE_PATH = r\"C:\\Users\\acer\\Downloads\\Enterprise_Sustainable Power Evaluation_Dataset.csv\"\n",
    "EMBEDDING_DIMENSION = 384  # Standard dimension for many sentence transformers (e.g., MiniLM)\n",
    "\n",
    "# Define the target variable (Emissions Intensity is a key EPM metric)\n",
    "TARGET_COLUMN = 'Emissions Intensity (kg CO₂ per MWh)'\n",
    "# Ensure this constant exactly matches the result of cleaning the TARGET_COLUMN\n",
    "TARGET_CLEANED = 'Emissions_Intensity_kg_CO2_per_MWh'\n",
    "\n",
    "# Features to use for prediction (other key EPM metrics)\n",
    "FEATURE_COLUMNS = [\n",
    "    'Revenue (USD)',\n",
    "    'Net Profit Margin (%)',\n",
    "    'Energy Efficiency (%)',\n",
    "    'Renewable Energy Share (%)',\n",
    "    'Sustainability Score',\n",
    "    'Innovation Index'\n",
    "]\n",
    "\n",
    "def load_and_clean_data(file_path):\n",
    "    \"\"\"Loads and cleans the initial CSV, standardizing column names.\"\"\"\n",
    "    print(\"--- 1. Loading and Cleaning Data ---\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found. Please ensure the file is accessible.\")\n",
    "        return None\n",
    "\n",
    "    # --- Robust Column Cleaning Fix ---\n",
    "    def standardize_column_name(col_name):\n",
    "        # 1. Start cleaning\n",
    "        cleaned = col_name\n",
    "        \n",
    "        # 2. Aggressively clean up known problematic strings (like CO₂ encoding issues)\n",
    "        cleaned = cleaned.replace('â\\x82\\x82', '2') # Handles one common encoding for CO₂\n",
    "        \n",
    "        # 3. Replace all non-alphanumeric, non-space characters with an underscore\n",
    "        cleaned = re.sub(r'[^A-Za-z0-9\\s_]', '', cleaned)\n",
    "        \n",
    "        # 4. Replace spaces with underscores\n",
    "        cleaned = cleaned.replace(' ', '_')\n",
    "        \n",
    "        # 5. Replace multiple underscores with a single underscore\n",
    "        cleaned = re.sub(r'_+', '_', cleaned)\n",
    "        \n",
    "        # 6. Strip any leading/trailing underscores\n",
    "        cleaned = cleaned.strip('_')\n",
    "        \n",
    "        # FIX: Explicitly handle the target column's known problematic output \n",
    "        # to guarantee the required name, as general regex cleaning is failing.\n",
    "        if 'Emissions_Intensity_kg_CO_per_MWh' in cleaned:\n",
    "            return TARGET_CLEANED\n",
    "        \n",
    "        return cleaned\n",
    "\n",
    "    # Apply the standardization to all columns\n",
    "    df.columns = [standardize_column_name(col) for col in df.columns]\n",
    "\n",
    "    # Convert the dataset into a single pseudo-time-series\n",
    "    df_ts = df.sort_values(by='Company_ID').reset_index(drop=True)\n",
    "    \n",
    "    # Generate the list of *correctly* clean feature columns using the same function\n",
    "    # Note: We now have to use the cleaned column names from the df.columns directly \n",
    "    # since the source string TARGET_COLUMN is unreliable.\n",
    "    # We will use the columns that are necessary.\n",
    "    \n",
    "    # List of all clean features and targets needed\n",
    "    required_cols = ['Company_ID', 'Revenue_USD', 'Net_Profit_Margin', 'Energy_Efficiency', \n",
    "                     'Renewable_Energy_Share', 'Sustainability_Score', 'Innovation_Index', \n",
    "                     TARGET_CLEANED]\n",
    "    \n",
    "    # Filter for the required columns\n",
    "    df_ts = df_ts[required_cols].dropna()\n",
    "    \n",
    "    # Validation check: Ensure the critical columns exist now\n",
    "    if TARGET_CLEANED not in df_ts.columns:\n",
    "        print(f\"\\nFATAL ERROR: Target column '{TARGET_CLEANED}' is still missing after cleaning.\")\n",
    "        print(f\"Available columns: {df_ts.columns.tolist()}\")\n",
    "        return None\n",
    "\n",
    "    # Sanity Check for the other columns used later in the script\n",
    "    if 'Energy_Efficiency' not in df_ts.columns or 'Innovation_Index' not in df_ts.columns:\n",
    "        print(\"\\nFATAL ERROR: Energy_Efficiency or Innovation_Index is missing after cleaning.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    print(f\"Dataset size after cleaning: {df_ts.shape}\")\n",
    "\n",
    "    return df_ts\n",
    "\n",
    "def simulate_narratives_and_embeddings(df):\n",
    "    \"\"\"\n",
    "    Simulates the LLM's role: generating contextual narratives and their embeddings.\n",
    "    Since we cannot run a live LLM, we use rule-based simulation.\n",
    "    \"\"\"\n",
    "    print(\"--- 2. Simulating Narrative Context and Embeddings ---\")\n",
    "    \n",
    "    # 1. Calculate step-wise changes in key indicators (simulating a time-series perspective)\n",
    "    # The columns here must match the names created in load_and_clean_data\n",
    "    df['Emissions_Change'] = df[TARGET_CLEANED].diff().fillna(0)\n",
    "    df['Efficiency_Change'] = df['Energy_Efficiency'].diff().fillna(0)\n",
    "    df['Innovation_Change'] = df['Innovation_Index'].diff().fillna(0)\n",
    "    \n",
    "    narratives = []\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        emissions_change = df.loc[i, 'Emissions_Change']\n",
    "        efficiency_change = df.loc[i, 'Efficiency_Change']\n",
    "        innovation_change = df.loc[i, 'Innovation_Change']\n",
    "        \n",
    "        narrative = \"\"\n",
    "\n",
    "        # --- Rule-Based Narrative Generation ---\n",
    "        \n",
    "        # A. Significant Emissions Improvement (Negative change is good)\n",
    "        if emissions_change < -50:\n",
    "            narrative = \"Following the deployment of a new carbon capture pilot program, the intensity of emissions saw a strong, unexpected decrease. Management expects this trend to stabilize next quarter, pending full-scale operational review.\"\n",
    "        # B. Emissions Spike (Positive change is bad)\n",
    "        elif emissions_change > 50:\n",
    "            narrative = \"Operational downtime at the primary renewable facility forced a temporary reliance on legacy assets, causing a sharp, but predicted, spike in emissions intensity. This is a short-term impact only.\"\n",
    "        # C. Efficiency Drop\n",
    "        elif efficiency_change < -5:\n",
    "            narrative = \"Initial reports indicate supply chain disruptions affecting key machinery maintenance, leading to a temporary decline in reported energy efficiency. Remedial efforts are underway.\"\n",
    "        # D. Innovation/Future Investment\n",
    "        elif innovation_change > 10:\n",
    "            narrative = \"Significant capital was allocated towards future-proofing and R&D for grid optimization, signalling a forward-looking strategy that may impact short-term profit margins but promises substantial long-term gains in sustainability.\"\n",
    "        # E. Baseline/Steady State\n",
    "        else:\n",
    "            narrative = \"Quarterly review shows stable performance across core metrics with no material changes to operational forecasts. The strategic focus remains on incremental improvements in resource allocation efficiency.\"\n",
    "\n",
    "        narratives.append(narrative)\n",
    "\n",
    "    df['Narrative'] = narratives\n",
    "    \n",
    "    # --- 3. Simulated Embedding Generation (Performance Fix) ---\n",
    "    # Here, we simulate the embeddings using random vectors for simplicity\n",
    "    print(f\"Simulating {len(df)} embeddings of dimension {EMBEDDING_DIMENSION}.\")\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create a dummy array of embeddings (0 to 1, consistent with normalized LLM embeddings)\n",
    "    embeddings = np.random.rand(len(df), EMBEDDING_DIMENSION).astype(np.float32)\n",
    "    \n",
    "    # FIX: Create a DataFrame from the embeddings array and concatenate it with the main DataFrame\n",
    "    # This replaces the slow iterative column insertion and avoids the PerformanceWarning.\n",
    "    embedding_cols = [f'Embedding_{j}' for j in range(EMBEDDING_DIMENSION)]\n",
    "    embeddings_df = pd.DataFrame(embeddings, columns=embedding_cols, index=df.index)\n",
    "    \n",
    "    # Join the embeddings back to the main DataFrame efficiently\n",
    "    df = pd.concat([df, embeddings_df], axis=1)\n",
    "        \n",
    "    # Drop intermediate change columns\n",
    "    df = df.drop(columns=['Emissions_Change', 'Efficiency_Change', 'Innovation_Change'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_augmented_data(df):\n",
    "    \"\"\"Saves the final DataFrame containing structured data, narrative, and embeddings.\"\"\"\n",
    "    AUGMENTED_FILE = 'epm_augmented_data_with_embeddings.csv'\n",
    "    df.to_csv(AUGMENTED_FILE, index=False)\n",
    "    print(f\"\\n--- 3. Data Augmentation Complete ---\")\n",
    "    print(f\"Saved augmented dataset to: {AUGMENTED_FILE}\")\n",
    "    print(f\"Final shape: {df.shape} (Includes {EMBEDDING_DIMENSION} embedding columns)\")\n",
    "    print(\"This file is now ready for the Hybrid Model (Phase 3).\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # The actual path to the accessible file is the string defined in FILE_PATH, \n",
    "    # not the temporary C:\\Users path, which is only used for tracking the original upload.\n",
    "    # We must pass the platform-accessible filename here.\n",
    "    df_ts_cleaned = load_and_clean_data(FILE_PATH) \n",
    "    if df_ts_cleaned is not None:\n",
    "        df_augmented = simulate_narratives_and_embeddings(df_ts_cleaned)\n",
    "        save_augmented_data(df_augmented)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f826616-9f4f-4051-a66e-26d69f5f505d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Loading and Narrative Simulation ---\n",
      "\n",
      "Training Samples: 792, Testing Samples: 198\n",
      "TS Input Shape (Train): (792, 10, 6)\n",
      "Narrative Input Shape (Train): (792, 384)\n",
      "\n",
      "--- 2. Building Dual-Input Hybrid Fusion Model ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"Holistic_Horizon_EPM_Hybrid\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"Holistic_Horizon_EPM_Hybrid\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ time_series_input             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                  │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,176</span> │ time_series_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ narrative_embedding_input     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                  │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ lstm_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │          <span style=\"color: #00af00; text-decoration-color: #00af00\">24,640</span> │ narrative_embedding_input… │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │          <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │ dropout_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ ts_feature_vector (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                │             <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │ lstm_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]               │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ narrative_feature_vector      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,040</span> │ dropout_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                       │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ fusion_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ ts_feature_vector[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
       "│                               │                           │                 │ narrative_feature_vector[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)                │             <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │ fusion_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ emissions_prediction (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                 │              <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]              │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ time_series_input             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m6\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)                  │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │          \u001b[38;5;34m18,176\u001b[0m │ time_series_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ narrative_embedding_input     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m384\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)                  │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │ lstm_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │          \u001b[38;5;34m24,640\u001b[0m │ narrative_embedding_input… │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │          \u001b[38;5;34m12,416\u001b[0m │ dropout_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ ts_feature_vector (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                │             \u001b[38;5;34m528\u001b[0m │ lstm_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]               │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ narrative_feature_vector      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                │           \u001b[38;5;34m1,040\u001b[0m │ dropout_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)                       │                           │                 │                            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ fusion_layer (\u001b[38;5;33mConcatenate\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │ ts_feature_vector[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
       "│                               │                           │                 │ narrative_feature_vector[\u001b[38;5;34m…\u001b[0m │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)                │             \u001b[38;5;34m528\u001b[0m │ fusion_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ emissions_prediction (\u001b[38;5;33mDense\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                 │              \u001b[38;5;34m17\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]              │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">57,345</span> (224.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m57,345\u001b[0m (224.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">57,345</span> (224.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m57,345\u001b[0m (224.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Training Hybrid Model ---\n",
      "Epoch 1/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 104ms/step - loss: 0.1526 - mae: 0.3135 - val_loss: 0.0831 - val_mae: 0.2385\n",
      "Epoch 2/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 54ms/step - loss: 0.0970 - mae: 0.2626 - val_loss: 0.0805 - val_mae: 0.2311\n",
      "Epoch 3/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 0.0899 - mae: 0.2539 - val_loss: 0.0822 - val_mae: 0.2328\n",
      "Epoch 4/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 127ms/step - loss: 0.0874 - mae: 0.2532 - val_loss: 0.0797 - val_mae: 0.2333\n",
      "Epoch 5/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 42ms/step - loss: 0.0868 - mae: 0.2527 - val_loss: 0.0795 - val_mae: 0.2342\n",
      "Epoch 6/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 114ms/step - loss: 0.0843 - mae: 0.2494 - val_loss: 0.0797 - val_mae: 0.2326\n",
      "Epoch 7/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0843 - mae: 0.2484 - val_loss: 0.0811 - val_mae: 0.2316\n",
      "Epoch 8/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0856 - mae: 0.2500 - val_loss: 0.0777 - val_mae: 0.2309\n",
      "Epoch 9/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0803 - mae: 0.2433 - val_loss: 0.0794 - val_mae: 0.2338\n",
      "Epoch 10/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0822 - mae: 0.2445 - val_loss: 0.0785 - val_mae: 0.2309\n",
      "Epoch 11/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0783 - mae: 0.2383 - val_loss: 0.0817 - val_mae: 0.2300\n",
      "Epoch 12/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0803 - mae: 0.2426 - val_loss: 0.0805 - val_mae: 0.2353\n",
      "Epoch 13/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0769 - mae: 0.2370 - val_loss: 0.0842 - val_mae: 0.2370\n",
      "Epoch 14/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0742 - mae: 0.2308 - val_loss: 0.0814 - val_mae: 0.2352\n",
      "Epoch 15/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0701 - mae: 0.2233 - val_loss: 0.0838 - val_mae: 0.2369\n",
      "Epoch 16/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0687 - mae: 0.2221 - val_loss: 0.0926 - val_mae: 0.2482\n",
      "Epoch 17/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0695 - mae: 0.2229 - val_loss: 0.0875 - val_mae: 0.2423\n",
      "Epoch 18/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0660 - mae: 0.2160 - val_loss: 0.0839 - val_mae: 0.2375\n",
      "Epoch 19/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0659 - mae: 0.2118 - val_loss: 0.0857 - val_mae: 0.2405\n",
      "Epoch 20/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0610 - mae: 0.2055 - val_loss: 0.0901 - val_mae: 0.2440\n",
      "Epoch 21/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0594 - mae: 0.1990 - val_loss: 0.0911 - val_mae: 0.2468\n",
      "Epoch 22/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0576 - mae: 0.1974 - val_loss: 0.0928 - val_mae: 0.2488\n",
      "Epoch 23/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0554 - mae: 0.1927 - val_loss: 0.0903 - val_mae: 0.2495\n",
      "Epoch 24/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0571 - mae: 0.1957 - val_loss: 0.0915 - val_mae: 0.2493\n",
      "Epoch 25/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0540 - mae: 0.1909 - val_loss: 0.0869 - val_mae: 0.2450\n",
      "Epoch 26/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0497 - mae: 0.1836 - val_loss: 0.0903 - val_mae: 0.2481\n",
      "Epoch 27/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0504 - mae: 0.1818 - val_loss: 0.0867 - val_mae: 0.2443\n",
      "Epoch 28/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0460 - mae: 0.1749 - val_loss: 0.0975 - val_mae: 0.2572\n",
      "Epoch 29/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0441 - mae: 0.1701 - val_loss: 0.0954 - val_mae: 0.2569\n",
      "Epoch 30/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0436 - mae: 0.1666 - val_loss: 0.1010 - val_mae: 0.2624\n",
      "Epoch 31/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0403 - mae: 0.1639 - val_loss: 0.0942 - val_mae: 0.2499\n",
      "Epoch 32/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0397 - mae: 0.1589 - val_loss: 0.0996 - val_mae: 0.2555\n",
      "Epoch 33/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0432 - mae: 0.1681 - val_loss: 0.1034 - val_mae: 0.2604\n",
      "Epoch 34/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0361 - mae: 0.1522 - val_loss: 0.1012 - val_mae: 0.2607\n",
      "Epoch 35/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0369 - mae: 0.1562 - val_loss: 0.1052 - val_mae: 0.2663\n",
      "Epoch 36/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - loss: 0.0364 - mae: 0.1515 - val_loss: 0.1064 - val_mae: 0.2695\n",
      "Epoch 37/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0346 - mae: 0.1491 - val_loss: 0.1053 - val_mae: 0.2660\n",
      "Epoch 38/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.0340 - mae: 0.1489 - val_loss: 0.1096 - val_mae: 0.2730\n",
      "Epoch 39/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0362 - mae: 0.1496 - val_loss: 0.1234 - val_mae: 0.2877\n",
      "Epoch 40/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0351 - mae: 0.1510 - val_loss: 0.1098 - val_mae: 0.2744\n",
      "Epoch 41/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0361 - mae: 0.1486 - val_loss: 0.1235 - val_mae: 0.2889\n",
      "Epoch 42/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0363 - mae: 0.1521 - val_loss: 0.1166 - val_mae: 0.2785\n",
      "Epoch 43/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - loss: 0.0392 - mae: 0.1591 - val_loss: 0.1270 - val_mae: 0.2932\n",
      "Epoch 44/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0401 - mae: 0.1636 - val_loss: 0.1261 - val_mae: 0.2934\n",
      "Epoch 45/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0416 - mae: 0.1666 - val_loss: 0.1269 - val_mae: 0.2933\n",
      "Epoch 46/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0506 - mae: 0.1806 - val_loss: 0.0909 - val_mae: 0.2491\n",
      "Epoch 47/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0460 - mae: 0.1775 - val_loss: 0.1031 - val_mae: 0.2688\n",
      "Epoch 48/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0379 - mae: 0.1577 - val_loss: 0.0954 - val_mae: 0.2621\n",
      "Epoch 49/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0379 - mae: 0.1584 - val_loss: 0.0854 - val_mae: 0.2427\n",
      "Epoch 50/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0410 - mae: 0.1661 - val_loss: 0.0996 - val_mae: 0.2649\n",
      "Epoch 51/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0388 - mae: 0.1605 - val_loss: 0.1145 - val_mae: 0.2837\n",
      "Epoch 52/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0409 - mae: 0.1626 - val_loss: 0.1087 - val_mae: 0.2733\n",
      "Epoch 53/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0348 - mae: 0.1493 - val_loss: 0.1164 - val_mae: 0.2796\n",
      "Epoch 54/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0305 - mae: 0.1391 - val_loss: 0.1136 - val_mae: 0.2798\n",
      "Epoch 55/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0303 - mae: 0.1401 - val_loss: 0.1063 - val_mae: 0.2685\n",
      "Epoch 56/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0313 - mae: 0.1383 - val_loss: 0.1151 - val_mae: 0.2780\n",
      "Epoch 57/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0307 - mae: 0.1365 - val_loss: 0.1152 - val_mae: 0.2845\n",
      "Epoch 58/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0284 - mae: 0.1340 - val_loss: 0.1134 - val_mae: 0.2813\n",
      "Epoch 59/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0281 - mae: 0.1306 - val_loss: 0.1159 - val_mae: 0.2824\n",
      "Epoch 60/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.0272 - mae: 0.1336 - val_loss: 0.1118 - val_mae: 0.2759\n",
      "Epoch 61/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0259 - mae: 0.1292 - val_loss: 0.1123 - val_mae: 0.2764\n",
      "Epoch 62/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0244 - mae: 0.1221 - val_loss: 0.1131 - val_mae: 0.2812\n",
      "Epoch 63/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0243 - mae: 0.1252 - val_loss: 0.1168 - val_mae: 0.2803\n",
      "Epoch 64/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0243 - mae: 0.1213 - val_loss: 0.1126 - val_mae: 0.2770\n",
      "Epoch 65/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0234 - mae: 0.1192 - val_loss: 0.1145 - val_mae: 0.2797\n",
      "Epoch 66/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0231 - mae: 0.1164 - val_loss: 0.1246 - val_mae: 0.2913\n",
      "Epoch 67/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0238 - mae: 0.1212 - val_loss: 0.1240 - val_mae: 0.2892\n",
      "Epoch 68/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0235 - mae: 0.1238 - val_loss: 0.1124 - val_mae: 0.2733\n",
      "Epoch 69/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0221 - mae: 0.1183 - val_loss: 0.1266 - val_mae: 0.2870\n",
      "Epoch 70/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0216 - mae: 0.1127 - val_loss: 0.1261 - val_mae: 0.2906\n",
      "Epoch 71/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0213 - mae: 0.1112 - val_loss: 0.1233 - val_mae: 0.2908\n",
      "Epoch 72/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0227 - mae: 0.1162 - val_loss: 0.1314 - val_mae: 0.2996\n",
      "Epoch 73/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0223 - mae: 0.1169 - val_loss: 0.1354 - val_mae: 0.2967\n",
      "Epoch 74/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 0.0271 - mae: 0.1293 - val_loss: 0.1204 - val_mae: 0.2810\n",
      "Epoch 75/75\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0244 - mae: 0.1235 - val_loss: 0.1201 - val_mae: 0.2797\n",
      "\n",
      "--- 4. Evaluating Hybrid Model Performance ---\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 120ms/step\n",
      "\n",
      "--- Holistic Horizon Hybrid Model Final Evaluation ---\n",
      "Target Variable: Emissions Intensity (kg CO₂ per MWh)\n",
      "Test MAE (Emissions Intensity): 0.29 kg CO₂ per MWh\n",
      "\n",
      "Next Steps (Phase 4): Compare this MAE directly to the Phase 1 Baseline to quantify the value of the LLM narrative context.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# --- Configuration (MUST match Phases 1 & 2) ---\n",
    "FILE_PATH = r\"C:\\Users\\acer\\Downloads\\Enterprise_Sustainable Power Evaluation_Dataset.csv\"\n",
    "SEQUENCE_LENGTH = 10        # Historical steps for Time-Series input\n",
    "EMBEDDING_DIMENSION = 384   # Dimensionality of the Narrative Embedding\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 75                 # Increased epochs for better convergence of complex model\n",
    "\n",
    "# Define the target variable\n",
    "TARGET_COLUMN = 'Emissions Intensity (kg CO₂ per MWh)'\n",
    "TARGET_CLEANED = 'Emissions_Intensity_kg_CO2_per_MWh'\n",
    "\n",
    "# Features to use for prediction (Time-Series path input)\n",
    "FEATURE_COLUMNS = [\n",
    "    'Revenue (USD)',\n",
    "    'Net Profit Margin (%)',\n",
    "    'Energy Efficiency (%)',\n",
    "    'Renewable Energy Share (%)',\n",
    "    'Sustainability Score',\n",
    "    'Innovation Index'\n",
    "]\n",
    "\n",
    "# --- Combined Data Preparation and Simulation (Replicating Phase 1 & 2 logic) ---\n",
    "\n",
    "def generate_augmented_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads, cleans data, and simulates the LLM narratives and embeddings.\n",
    "    (This function ensures the model script is self-contained and runnable).\n",
    "    \"\"\"\n",
    "    print(\"--- 1. Data Loading and Narrative Simulation ---\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}. Please ensure the file is in the correct directory.\")\n",
    "        return None, None\n",
    "\n",
    "    # --- Robust Column Cleaning Fix (Copied from working Phase 2 logic) ---\n",
    "    def standardize_column_name(col_name):\n",
    "        # 1. Start cleaning\n",
    "        cleaned = col_name\n",
    "        \n",
    "        # 2. Aggressively clean up known problematic strings (like CO₂ encoding issues)\n",
    "        cleaned = cleaned.replace('â\\x82\\x82', '2') # Handles one common encoding for CO₂\n",
    "        \n",
    "        # 3. Replace all non-alphanumeric, non-space characters with an underscore\n",
    "        cleaned = re.sub(r'[^A-Za-z0-9\\s_]', '', cleaned)\n",
    "        \n",
    "        # 4. Replace spaces with underscores\n",
    "        cleaned = cleaned.replace(' ', '_')\n",
    "        \n",
    "        # 5. Replace multiple underscores with a single underscore\n",
    "        cleaned = re.sub(r'_+', '_', cleaned)\n",
    "        \n",
    "        # 6. Strip any leading/trailing underscores\n",
    "        cleaned = cleaned.strip('_')\n",
    "        \n",
    "        # FIX: Explicitly handle the target column's known problematic output \n",
    "        # to guarantee the required name, as general regex cleaning is failing.\n",
    "        if 'Emissions_Intensity_kg_CO_per_MWh' in cleaned:\n",
    "            return TARGET_CLEANED\n",
    "        \n",
    "        return cleaned\n",
    "\n",
    "    # Apply the standardization to all columns\n",
    "    df.columns = [standardize_column_name(col) for col in df.columns]\n",
    "\n",
    "    # Convert the dataset into a single pseudo-time-series\n",
    "    df_ts = df.sort_values(by='Company_ID').reset_index(drop=True)\n",
    "    \n",
    "    # List of all clean features and targets needed\n",
    "    required_cols = ['Company_ID', 'Revenue_USD', 'Net_Profit_Margin', 'Energy_Efficiency', \n",
    "                     'Renewable_Energy_Share', 'Sustainability_Score', 'Innovation_Index', \n",
    "                     TARGET_CLEANED]\n",
    "    \n",
    "    # Filter for the required columns\n",
    "    # We now use 'df_ts' directly, which has the cleaned columns from the step above.\n",
    "    df_ts = df_ts[required_cols].dropna()\n",
    "\n",
    "    # --- End of Cleaning Fix ---\n",
    "\n",
    "    # Calculate changes for rule-based narrative generation (Phase 2 logic)\n",
    "    df_ts['Emissions_Change'] = df_ts[TARGET_CLEANED].diff().fillna(0)\n",
    "    df_ts['Efficiency_Change'] = df_ts['Energy_Efficiency'].diff().fillna(0)\n",
    "    df_ts['Innovation_Change'] = df_ts['Innovation_Index'].diff().fillna(0)\n",
    "    \n",
    "    # Simulate embeddings (Phase 2 logic)\n",
    "    np.random.seed(42)\n",
    "    embeddings = np.random.rand(len(df_ts), EMBEDDING_DIMENSION).astype(np.float32)\n",
    "    \n",
    "    # FIX: Use efficient concatenation to avoid PerformanceWarning\n",
    "    embedding_cols = [f'Embedding_{j}' for j in range(EMBEDDING_DIMENSION)]\n",
    "    embeddings_df = pd.DataFrame(embeddings, columns=embedding_cols, index=df_ts.index)\n",
    "    df_ts = pd.concat([df_ts, embeddings_df], axis=1)\n",
    "\n",
    "    df_ts = df_ts.drop(columns=['Emissions_Change', 'Efficiency_Change', 'Innovation_Change', 'Company_ID'])\n",
    "    \n",
    "    # Identify feature and embedding columns\n",
    "    ts_features = [col for col in df_ts.columns if col.startswith(('Revenue', 'Net_Profit', 'Energy_Efficiency', 'Renewable', 'Sustainability_Score', 'Innovation_Index'))]\n",
    "    # The embedding features start after the TS features\n",
    "    num_ts_features = len(ts_features)\n",
    "\n",
    "    # Final data scaling and structuring\n",
    "    data = df_ts.values\n",
    "    \n",
    "    # 1. Scale Features\n",
    "    scaler_features = MinMaxScaler()\n",
    "    features_scaled = scaler_features.fit_transform(data[:, :-1]) # All except target\n",
    "    \n",
    "    # 2. Scale Target\n",
    "    scaler_target = MinMaxScaler()\n",
    "    target_scaled = scaler_target.fit_transform(data[:, -1].reshape(-1, 1))\n",
    "\n",
    "    # Recombine scaled data\n",
    "    scaled_data = np.hstack((features_scaled, target_scaled))\n",
    "    \n",
    "    return scaled_data, num_ts_features, scaler_target\n",
    "\n",
    "\n",
    "def create_hybrid_sequences(data, sequence_length, num_ts_features):\n",
    "    \"\"\"\n",
    "    Creates sequences for the dual-input model.\n",
    "    X_ts: Time-series features (multi-step history)\n",
    "    X_narrative: Narrative embedding (single step at prediction time)\n",
    "    y: Target value (single step at prediction time)\n",
    "    \"\"\"\n",
    "    X_ts, X_narrative, y = [], [], []\n",
    "    \n",
    "    # Number of total non-target columns (TS Features + Embeddings)\n",
    "    num_total_features = data.shape[1] - 1 \n",
    "    \n",
    "    for i in range(len(data) - sequence_length):\n",
    "        # A. Time-Series Input (Historical sequence of TS Features only)\n",
    "        # Select rows [i to i + SEQUENCE_LENGTH - 1] and columns [0 to num_ts_features - 1]\n",
    "        X_ts.append(data[i: i + sequence_length, :num_ts_features])\n",
    "        \n",
    "        # B. Narrative Input (Embedding vector corresponding to the prediction step)\n",
    "        # Select row [i + SEQUENCE_LENGTH] and columns [num_ts_features to num_total_features - 1]\n",
    "        X_narrative.append(data[i + sequence_length, num_ts_features:num_total_features])\n",
    "        \n",
    "        # C. Target (Target value at the prediction step)\n",
    "        # Select row [i + SEQUENCE_LENGTH] and the last column\n",
    "        y.append(data[i + sequence_length, -1])\n",
    "\n",
    "    return np.array(X_ts), np.array(X_narrative), np.array(y)\n",
    "\n",
    "# --- 2. Hybrid Fusion Model Definition ---\n",
    "\n",
    "def build_hybrid_model(ts_input_shape, narrative_input_shape):\n",
    "    \"\"\"\n",
    "    Defines the Dual-Input Fusion Model architecture using Keras Functional API.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 2. Building Dual-Input Hybrid Fusion Model ---\")\n",
    "    \n",
    "    # 1. Time-Series Path (Quantitative)\n",
    "    ts_input = Input(shape=ts_input_shape, name='time_series_input')\n",
    "    lstm_1 = LSTM(units=64, return_sequences=True)(ts_input)\n",
    "    dropout_ts_1 = Dropout(0.3)(lstm_1)\n",
    "    lstm_2 = LSTM(units=32)(dropout_ts_1)\n",
    "    ts_output = Dense(16, activation='relu', name='ts_feature_vector')(lstm_2)\n",
    "    \n",
    "    # 2. Narrative Path (Qualitative)\n",
    "    narrative_input = Input(shape=narrative_input_shape, name='narrative_embedding_input')\n",
    "    dense_narrative_1 = Dense(64, activation='relu')(narrative_input)\n",
    "    dropout_narrative_1 = Dropout(0.3)(dense_narrative_1)\n",
    "    narrative_output = Dense(16, activation='relu', name='narrative_feature_vector')(dropout_narrative_1)\n",
    "    \n",
    "    # 3. Fusion Layer (Concatenation)\n",
    "    fusion_layer = Concatenate(name='fusion_layer')([ts_output, narrative_output])\n",
    "    \n",
    "    # 4. Final Prediction Head\n",
    "    dense_final_1 = Dense(16, activation='relu')(fusion_layer)\n",
    "    output = Dense(1, activation='linear', name='emissions_prediction')(dense_final_1)\n",
    "    \n",
    "    # Define the final model\n",
    "    model = Model(inputs=[ts_input, narrative_input], outputs=output, name='Holistic_Horizon_EPM_Hybrid')\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 1. Prepare and structure data\n",
    "    scaled_data, num_ts_features, scaler_target = generate_augmented_data(FILE_PATH)\n",
    "    \n",
    "    if scaled_data is None:\n",
    "        exit()\n",
    "\n",
    "    X_ts, X_narrative, y = create_hybrid_sequences(scaled_data, SEQUENCE_LENGTH, num_ts_features)\n",
    "\n",
    "    # 2. Split into train and test sets\n",
    "    test_size = 0.2\n",
    "    split_index = int(len(X_ts) * (1 - test_size))\n",
    "\n",
    "    X_ts_train, X_ts_test = X_ts[:split_index], X_ts[split_index:]\n",
    "    X_narrative_train, X_narrative_test = X_narrative[:split_index], X_narrative[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "    print(f\"\\nTraining Samples: {len(y_train)}, Testing Samples: {len(y_test)}\")\n",
    "    print(f\"TS Input Shape (Train): {X_ts_train.shape}\")\n",
    "    print(f\"Narrative Input Shape (Train): {X_narrative_train.shape}\")\n",
    "\n",
    "    # 3. Build and train the model\n",
    "    ts_input_shape = (X_ts_train.shape[1], X_ts_train.shape[2])\n",
    "    narrative_input_shape = (X_narrative_train.shape[1],)\n",
    "    \n",
    "    hybrid_model = build_hybrid_model(ts_input_shape, narrative_input_shape)\n",
    "    hybrid_model.summary()\n",
    "    \n",
    "    print(\"\\n--- 3. Training Hybrid Model ---\")\n",
    "    history = hybrid_model.fit(\n",
    "        {'time_series_input': X_ts_train, 'narrative_embedding_input': X_narrative_train},\n",
    "        y_train,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_split=0.1,\n",
    "        verbose=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # 4. Evaluate and Inverse Transform\n",
    "    print(\"\\n--- 4. Evaluating Hybrid Model Performance ---\")\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred_scaled = hybrid_model.predict({'time_series_input': X_ts_test, 'narrative_embedding_input': X_narrative_test})\n",
    "    \n",
    "    # Inverse transform to get the prediction in the original scale\n",
    "    y_test_original = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "    y_pred_original = scaler_target.inverse_transform(y_pred_scaled)\n",
    "    \n",
    "    # Calculate Mean Absolute Error (MAE)\n",
    "    mae = np.mean(np.abs(y_pred_original - y_test_original))\n",
    "    \n",
    "    print(f\"\\n--- Holistic Horizon Hybrid Model Final Evaluation ---\")\n",
    "    print(f\"Target Variable: {TARGET_COLUMN}\")\n",
    "    print(f\"Test MAE (Emissions Intensity): {mae:.2f} kg CO₂ per MWh\")\n",
    "    print(\"\\nNext Steps (Phase 4): Compare this MAE directly to the Phase 1 Baseline to quantify the value of the LLM narrative context.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1079804-074d-4f6c-b8b9-838c5c35f028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Data Preparation and Simulation ---\n",
      "\n",
      "Training Samples: 712, Validation Samples: 80, Testing Samples: 198\n",
      "\n",
      "--- Training LSTM Baseline Model ---\n",
      "\n",
      "--- Evaluation: LSTM Baseline Model ---\n",
      "Test MAE (Emissions Intensity (kg CO₂ per MWh)): 0.2513 kg CO₂ per MWh\n",
      "\n",
      "--- Training Holistic Horizon Hybrid Model ---\n",
      "\n",
      "--- Evaluation: Hybrid Fusion Model ---\n",
      "Test MAE (Emissions Intensity (kg CO₂ per MWh)): 0.2964 kg CO₂ per MWh\n",
      "\n",
      "==================================================\n",
      "      Holistic Horizon EPM Project: Final Comparison\n",
      "==================================================\n",
      "Target Metric: Emissions Intensity (kg CO₂ per MWh)\n",
      "1. Pure Time-Series (LSTM Baseline) MAE: 0.2513\n",
      "2. Multi-Modal Hybrid (Fusion) MAE:       0.2964\n",
      "\n",
      "Conclusion: Hybrid model underperformed the Baseline by 17.96%.\n",
      "This suggests that the simulated narrative context might be introducing noise or that the current fusion architecture needs tuning.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# --- Configuration (MUST match previous phases) ---\n",
    "FILE_PATH = r\"C:\\Users\\acer\\Downloads\\Enterprise_Sustainable Power Evaluation_Dataset.csv\"\n",
    "SEQUENCE_LENGTH = 10        # Historical steps for Time-Series input\n",
    "EMBEDDING_DIMENSION = 384   # Dimensionality of the Narrative Embedding\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 75                 # Consistent training length for fair comparison\n",
    "\n",
    "# Define the target variable\n",
    "TARGET_COLUMN = 'Emissions Intensity (kg CO₂ per MWh)'\n",
    "TARGET_CLEANED = 'Emissions_Intensity_kg_CO2_per_MWh'\n",
    "\n",
    "# Features to use for prediction (Time-Series path input)\n",
    "FEATURE_COLUMNS = [\n",
    "    'Revenue (USD)',\n",
    "    'Net Profit Margin (%)',\n",
    "    'Energy Efficiency (%)',\n",
    "    'Renewable Energy Share (%)',\n",
    "    'Sustainability Score',\n",
    "    'Innovation Index'\n",
    "]\n",
    "\n",
    "# --- Helper function for robust column cleaning ---\n",
    "def standardize_column_name_robust(col_name):\n",
    "    \"\"\"Aggressively cleans and standardizes a column name.\"\"\"\n",
    "    cleaned = col_name\n",
    "    \n",
    "    # 1. Handle known encoding/symbol issues (e.g., CO₂ -> CO2)\n",
    "    cleaned = cleaned.replace('â\\x82\\x82', '2') \n",
    "    \n",
    "    # 2. Replace all non-alphanumeric, non-space characters with an underscore\n",
    "    cleaned = re.sub(r'[^A-Za-z0-9\\s_]', '_', cleaned)\n",
    "    \n",
    "    # 3. Replace spaces with underscores\n",
    "    cleaned = cleaned.replace(' ', '_')\n",
    "    \n",
    "    # 4. Collapse multiple underscores and strip leading/trailing ones\n",
    "    cleaned = re.sub(r'_+', '_', cleaned).strip('_')\n",
    "    \n",
    "    # 5. FIX: Ensure the target column is mapped correctly due to unpredictable source reading\n",
    "    if 'Emissions_Intensity_kg_CO_per_MWh' in cleaned:\n",
    "        return TARGET_CLEANED\n",
    "        \n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# --- Data Preparation and Simulation ---\n",
    "\n",
    "def generate_augmented_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads, cleans, simulates narratives/embeddings, scales data, and returns \n",
    "    the necessary components for sequence creation.\n",
    "    \"\"\"\n",
    "    print(\"--- 1. Data Preparation and Simulation ---\")\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}.\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Apply the single, robust standardization function to all columns\n",
    "    df.columns = [standardize_column_name_robust(col) for col in df.columns]\n",
    "\n",
    "    # Convert the dataset into a single pseudo-time-series\n",
    "    df_ts = df.sort_values(by='Company_ID').reset_index(drop=True)\n",
    "\n",
    "    # The list of columns to use is now based on the standardized names\n",
    "    clean_feature_cols = [standardize_column_name_robust(col) for col in FEATURE_COLUMNS]\n",
    "    required_cols = ['Company_ID'] + clean_feature_cols + [TARGET_CLEANED]\n",
    "\n",
    "    # Filter for the required columns\n",
    "    # This step will now use the consistently clean column names\n",
    "    df_ts = df_ts[required_cols].dropna()\n",
    "\n",
    "    # Identify the feature columns present in the final DataFrame\n",
    "    ts_features = [col for col in df_ts.columns if col in clean_feature_cols]\n",
    "\n",
    "    # Calculate changes for narrative simulation (using a generic proxy, as full simulation is complex)\n",
    "    df_ts['Emissions_Change'] = df_ts[TARGET_CLEANED].diff().fillna(0)\n",
    "    \n",
    "    # Simulate embeddings\n",
    "    np.random.seed(42)\n",
    "    embeddings = np.random.rand(len(df_ts), EMBEDDING_DIMENSION).astype(np.float32)\n",
    "    \n",
    "    # Use efficient concatenation to add embedding columns (avoiding PerformanceWarning)\n",
    "    embedding_cols = [f'Embedding_{j}' for j in range(EMBEDDING_DIMENSION)]\n",
    "    embeddings_df = pd.DataFrame(embeddings, columns=embedding_cols, index=df_ts.index)\n",
    "    df_ts = pd.concat([df_ts, embeddings_df], axis=1)\n",
    "\n",
    "    # Drop non-feature columns\n",
    "    df_ts = df_ts.drop(columns=[col for col in df_ts.columns if 'Change' in col] + ['Company_ID'])\n",
    "    \n",
    "    # Final data scaling and structuring\n",
    "    data = df_ts.values\n",
    "    num_ts_features = len(ts_features)\n",
    "    \n",
    "    # 1. Scale Features (all except target)\n",
    "    scaler_features = MinMaxScaler()\n",
    "    features_scaled = scaler_features.fit_transform(data[:, :-1])\n",
    "    \n",
    "    # 2. Scale Target\n",
    "    scaler_target = MinMaxScaler()\n",
    "    target_scaled = scaler_target.fit_transform(data[:, -1].reshape(-1, 1))\n",
    "\n",
    "    # Recombine scaled data\n",
    "    scaled_data = np.hstack((features_scaled, target_scaled))\n",
    "    \n",
    "    return scaled_data, num_ts_features, scaler_target\n",
    "\n",
    "\n",
    "def create_sequences(data, sequence_length, num_ts_features):\n",
    "    \"\"\"\n",
    "    Creates sequences for the dual-input model (Hybrid) and single-input (Baseline).\n",
    "    Returns X_ts, X_narrative, y.\n",
    "    \"\"\"\n",
    "    X_ts, X_narrative, y = [], [], []\n",
    "    num_total_features = data.shape[1] - 1 \n",
    "    \n",
    "    for i in range(len(data) - sequence_length):\n",
    "        # A. Time-Series Input (Historical sequence of TS Features only)\n",
    "        # Columns [0 to num_ts_features - 1]\n",
    "        X_ts.append(data[i: i + sequence_length, :num_ts_features])\n",
    "        \n",
    "        # B. Narrative Input (Embedding vector at prediction step)\n",
    "        # Columns [num_ts_features to num_total_features - 1]\n",
    "        X_narrative.append(data[i + sequence_length, num_ts_features:num_total_features])\n",
    "        \n",
    "        # C. Target (Target value at prediction step)\n",
    "        y.append(data[i + sequence_length, -1])\n",
    "\n",
    "    return np.array(X_ts, dtype=np.float32), np.array(X_narrative, dtype=np.float32), np.array(y, dtype=np.float32)\n",
    "\n",
    "# --- TensorFlow Dataset Creation ---\n",
    "\n",
    "def create_tf_datasets(X_ts_train, X_ts_val, X_ts_test, X_narrative_train, X_narrative_val, X_narrative_test, y_train, y_val, y_test, batch_size):\n",
    "    \"\"\"Creates optimized TensorFlow Dataset objects for training and testing, incorporating validation set.\"\"\"\n",
    "    \n",
    "    # Baseline Dataset (Single Input: Time-Series only)\n",
    "    ds_baseline_train = tf.data.Dataset.from_tensor_slices((X_ts_train, y_train))\n",
    "    ds_baseline_val = tf.data.Dataset.from_tensor_slices((X_ts_val, y_val))\n",
    "    ds_baseline_test = tf.data.Dataset.from_tensor_slices((X_ts_test, y_test))\n",
    "    \n",
    "    # Hybrid Dataset (Dual Input: Time-Series and Narrative)\n",
    "    X_train_hybrid = {'time_series_input': X_ts_train, 'narrative_embedding_input': X_narrative_train}\n",
    "    X_val_hybrid = {'time_series_input': X_ts_val, 'narrative_embedding_input': X_narrative_val}\n",
    "    X_test_hybrid = {'time_series_input': X_ts_test, 'narrative_embedding_input': X_narrative_test}\n",
    "    \n",
    "    ds_hybrid_train = tf.data.Dataset.from_tensor_slices((X_train_hybrid, y_train))\n",
    "    ds_hybrid_val = tf.data.Dataset.from_tensor_slices((X_val_hybrid, y_val))\n",
    "    ds_hybrid_test = tf.data.Dataset.from_tensor_slices((X_test_hybrid, y_test))\n",
    "\n",
    "    # Apply batching and prefetching for performance\n",
    "    ds_baseline_train = ds_baseline_train.shuffle(buffer_size=1024).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    ds_baseline_val = ds_baseline_val.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    ds_baseline_test = ds_baseline_test.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    ds_hybrid_train = ds_hybrid_train.shuffle(buffer_size=1024).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    ds_hybrid_val = ds_hybrid_val.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    ds_hybrid_test = ds_hybrid_test.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return (ds_baseline_train, ds_baseline_val, ds_baseline_test), (ds_hybrid_train, ds_hybrid_val, ds_hybrid_test)\n",
    "\n",
    "\n",
    "# --- Model Definitions ---\n",
    "\n",
    "def build_baseline_model(ts_input_shape):\n",
    "    \"\"\"LSTM Baseline Model (Phase 1).\"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=ts_input_shape, name='ts_baseline_input'), # Use Input layer to avoid warning\n",
    "        LSTM(units=50, return_sequences=True, name='lstm_baseline_1'),\n",
    "        Dropout(0.2),\n",
    "        LSTM(units=50, return_sequences=False, name='lstm_baseline_2'),\n",
    "        Dropout(0.2),\n",
    "        Dense(units=1, activation='linear', name='baseline_output')\n",
    "    ], name=\"EPM_LSTM_Baseline\")\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def build_hybrid_model(ts_input_shape, narrative_input_shape):\n",
    "    \"\"\"Hybrid Fusion Model (Phase 3).\"\"\"\n",
    "    # 1. Time-Series Path (Quantitative)\n",
    "    ts_input = Input(shape=ts_input_shape, name='time_series_input')\n",
    "    lstm_1 = LSTM(units=64, return_sequences=True)(ts_input)\n",
    "    dropout_ts_1 = Dropout(0.3)(lstm_1)\n",
    "    lstm_2 = LSTM(units=32)(dropout_ts_1)\n",
    "    ts_output = Dense(16, activation='relu', name='ts_feature_vector')(lstm_2)\n",
    "    \n",
    "    # 2. Narrative Path (Qualitative)\n",
    "    narrative_input = Input(shape=narrative_input_shape, name='narrative_embedding_input')\n",
    "    dense_narrative_1 = Dense(64, activation='relu')(narrative_input)\n",
    "    dropout_narrative_1 = Dropout(0.3)(dense_narrative_1)\n",
    "    narrative_output = Dense(16, activation='relu', name='narrative_feature_vector')(dropout_narrative_1)\n",
    "    \n",
    "    # 3. Fusion Layer\n",
    "    fusion_layer = Concatenate(name='fusion_layer')([ts_output, narrative_output])\n",
    "    \n",
    "    # 4. Final Prediction Head\n",
    "    dense_final_1 = Dense(16, activation='relu')(fusion_layer)\n",
    "    output = Dense(1, activation='linear', name='emissions_prediction')(dense_final_1)\n",
    "    \n",
    "    model = Model(inputs=[ts_input, narrative_input], outputs=output, name='Holistic_Horizon_EPM_Hybrid')\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# --- Evaluation Function ---\n",
    "\n",
    "def evaluate_model(model, ds_test, y_test_original_scale, scaler_target, model_name):\n",
    "    \"\"\"Evaluates the model using the TensorFlow Dataset.\"\"\"\n",
    "    \n",
    "    # Predict\n",
    "    y_pred_scaled = model.predict(ds_test, verbose=0)\n",
    "    \n",
    "    # Inverse transform\n",
    "    y_pred_original = scaler_target.inverse_transform(y_pred_scaled)\n",
    "    \n",
    "    # Calculate Mean Absolute Error (MAE)\n",
    "    mae = np.mean(np.abs(y_pred_original - y_test_original_scale))\n",
    "    \n",
    "    print(f\"\\n--- Evaluation: {model_name} ---\")\n",
    "    print(f\"Test MAE ({TARGET_COLUMN}): {mae:.4f} kg CO₂ per MWh\")\n",
    "    return mae\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 1. Prepare and structure data\n",
    "    scaled_data, num_ts_features, scaler_target = generate_augmented_data(FILE_PATH)\n",
    "    if scaled_data is None:\n",
    "        exit()\n",
    "\n",
    "    X_ts, X_narrative, y = create_sequences(scaled_data, SEQUENCE_LENGTH, num_ts_features)\n",
    "\n",
    "    # 2. Split into train, validation, and test sets (consistent split)\n",
    "    test_size_ratio = 0.2\n",
    "    val_size_ratio = 0.1 # 10% of the training data\n",
    "    \n",
    "    # Split into Train + Val and Test\n",
    "    split_index = int(len(X_ts) * (1 - test_size_ratio))\n",
    "    X_ts_tv, X_ts_test = X_ts[:split_index], X_ts[split_index:]\n",
    "    X_narrative_tv, X_narrative_test = X_narrative[:split_index], X_narrative[split_index:]\n",
    "    y_tv, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "    # Split Train + Val into Train and Val\n",
    "    val_index = int(len(X_ts_tv) * (1 - val_size_ratio))\n",
    "    X_ts_train, X_ts_val = X_ts_tv[:val_index], X_ts_tv[val_index:]\n",
    "    X_narrative_train, X_narrative_val = X_narrative_tv[:val_index], X_narrative_tv[val_index:]\n",
    "    y_train, y_val = y_tv[:val_index], y_tv[val_index:]\n",
    "    \n",
    "    # Get the test target values in the original scale for final evaluation \n",
    "    y_test_original_scale = scaler_target.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    print(f\"\\nTraining Samples: {len(y_train)}, Validation Samples: {len(y_val)}, Testing Samples: {len(y_test)}\")\n",
    "    \n",
    "    ts_input_shape = (X_ts_train.shape[1], X_ts_train.shape[2])\n",
    "    narrative_input_shape = (X_narrative_train.shape[1],)\n",
    "    \n",
    "    # 3. Create TensorFlow Datasets (Optimization step)\n",
    "    (ds_baseline_train, ds_baseline_val, ds_baseline_test), (ds_hybrid_train, ds_hybrid_val, ds_hybrid_test) = \\\n",
    "        create_tf_datasets(X_ts_train, X_ts_val, X_ts_test, X_narrative_train, X_narrative_val, X_narrative_test, y_train, y_val, y_test, BATCH_SIZE)\n",
    "    \n",
    "    # --- 4. Run Baseline Model ---\n",
    "    \n",
    "    baseline_model = build_baseline_model(ts_input_shape)\n",
    "    print(\"\\n--- Training LSTM Baseline Model ---\")\n",
    "    baseline_model.fit(\n",
    "        ds_baseline_train,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=ds_baseline_val,\n",
    "        verbose=0,\n",
    "    )\n",
    "    mae_baseline = evaluate_model(baseline_model, ds_baseline_test, y_test_original_scale, scaler_target, \"LSTM Baseline Model\")\n",
    "    \n",
    "    # --- 5. Run Hybrid Model ---\n",
    "\n",
    "    hybrid_model = build_hybrid_model(ts_input_shape, narrative_input_shape)\n",
    "    print(\"\\n--- Training Holistic Horizon Hybrid Model ---\")\n",
    "    hybrid_model.fit(\n",
    "        ds_hybrid_train,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=ds_hybrid_val,\n",
    "        verbose=0,\n",
    "    )\n",
    "    mae_hybrid = evaluate_model(hybrid_model, ds_hybrid_test, y_test_original_scale, scaler_target, \"Hybrid Fusion Model\")\n",
    "    \n",
    "    # --- 6. Comparative Analysis ---\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"      Holistic Horizon EPM Project: Final Comparison\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Target Metric: {TARGET_COLUMN}\")\n",
    "    print(f\"1. Pure Time-Series (LSTM Baseline) MAE: {mae_baseline:.4f}\")\n",
    "    print(f\"2. Multi-Modal Hybrid (Fusion) MAE:       {mae_hybrid:.4f}\")\n",
    "    \n",
    "    if mae_hybrid < mae_baseline:\n",
    "        improvement = ((mae_baseline - mae_hybrid) / mae_baseline) * 100\n",
    "        print(f\"\\nConclusion: Hybrid model outperformed the Baseline by {improvement:.2f}%.\")\n",
    "        print(\"This suggests that the **qualitative narrative context** (simulated LLM embeddings) adds significant predictive power and reduces forecast error.\")\n",
    "    elif mae_hybrid > mae_baseline:\n",
    "        decline = ((mae_hybrid - mae_baseline) / mae_baseline) * 100\n",
    "        print(f\"\\nConclusion: Hybrid model underperformed the Baseline by {decline:.2f}%.\")\n",
    "        print(\"This suggests that the simulated narrative context might be introducing noise or that the current fusion architecture needs tuning.\")\n",
    "    else:\n",
    "        print(\"\\nConclusion: The models performed identically. Further tuning is required.\")\n",
    "    \n",
    "    print(\"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74728128-d7fe-4413-826e-6964570c1d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 11:14:22.478 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
      "2025-10-07 11:14:22.482 WARNING streamlit.runtime.caching.cache_data_api: No runtime found, using MemoryCacheStorageManager\n",
      "2025-10-07 11:14:23.033 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\acer\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-10-07 11:14:23.038 No runtime found, using MemoryCacheStorageManager\n",
      "2025-10-07 11:14:23.078 No runtime found, using MemoryCacheStorageManager\n",
      "2025-10-07 11:14:25.104 Session state does not function when running a script without `streamlit run`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import re\n",
    "\n",
    "# --- Configuration (Matching the EPM project structure) ---\n",
    "# NOTE: Using the generic filename for platform compatibility\n",
    "FILE_PATH = r\"C:\\Users\\acer\\Downloads\\Enterprise_Sustainable Power Evaluation_Dataset.csv\"\n",
    "TARGET_COLUMN = 'Emissions Intensity (kg CO₂ per MWh)'\n",
    "TARGET_CLEANED = 'Emissions_Intensity_kg_CO2_per_MWh'\n",
    "\n",
    "# --- Robust Column Cleaning Function (Replicating ML analysis success) ---\n",
    "@st.cache_data\n",
    "def standardize_column_name_robust(col_name):\n",
    "    \"\"\"Aggressively cleans and standardizes a column name.\"\"\"\n",
    "    cleaned = col_name\n",
    "    \n",
    "    # 1. Handle known encoding/symbol issues (e.g., CO₂ -> CO2)\n",
    "    cleaned = cleaned.replace('â\\x82\\x82', '2') \n",
    "    \n",
    "    # 2. Replace all non-alphanumeric, non-space characters with an underscore\n",
    "    cleaned = re.sub(r'[^A-Za-z0-9\\s_]', '', cleaned)\n",
    "    \n",
    "    # 3. Replace spaces with underscores\n",
    "    cleaned = cleaned.replace(' ', '_')\n",
    "    \n",
    "    # 4. Collapse multiple underscores and strip leading/trailing ones\n",
    "    cleaned = re.sub(r'_+', '_', cleaned).strip('_')\n",
    "    \n",
    "    # 5. FIX: Ensure the target column is mapped correctly\n",
    "    if 'Emissions_Intensity_kg_CO_per_MWh' in cleaned:\n",
    "        return TARGET_CLEANED\n",
    "        \n",
    "    return cleaned\n",
    "\n",
    "# --- Data Loading and Cleaning ---\n",
    "@st.cache_data\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"Loads, cleans, and prepares the dataset.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except FileNotFoundError:\n",
    "        st.error(f\"Error: File not found at {file_path}. Please ensure the CSV is accessible.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Apply the single, robust standardization function to all columns\n",
    "    df.columns = [standardize_column_name_robust(col) for col in df.columns]\n",
    "\n",
    "    # Convert to pseudo-time-series by sorting\n",
    "    df_ts = df.sort_values(by='Company_ID').reset_index(drop=True)\n",
    "    \n",
    "    # Create a simple time index for plotting the simulated time-series\n",
    "    df_ts['Time_Step'] = df_ts.index\n",
    "    \n",
    "    # Ensure key columns are float for plotting\n",
    "    key_cols = [TARGET_CLEANED, 'Renewable_Energy_Share', 'Sustainability_Score', 'Net_Profit_Margin']\n",
    "    for col in key_cols:\n",
    "        if col in df_ts.columns:\n",
    "            df_ts[col] = pd.to_numeric(df_ts[col], errors='coerce')\n",
    "            \n",
    "    df_ts = df_ts.dropna()\n",
    "    return df_ts\n",
    "\n",
    "df = load_and_preprocess_data(FILE_PATH)\n",
    "\n",
    "# Check if data loaded successfully\n",
    "if df.empty:\n",
    "    st.stop()\n",
    "\n",
    "# --- Dashboard Layout and Styling ---\n",
    "st.set_page_config(\n",
    "    layout=\"wide\", \n",
    "    page_title=\"Holistic Horizon EPM Prediction Dashboard\", \n",
    "    initial_sidebar_state=\"collapsed\"\n",
    ")\n",
    "\n",
    "# Apply custom CSS for dark mode look and clean typography\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    "    .stApp {\n",
    "        background-color: #0d1117; /* Dark background */\n",
    "        color: #c9d1d9; /* Light text */\n",
    "    }\n",
    "    .stPlotly, .stAlert {\n",
    "        border-radius: 8px;\n",
    "        padding: 10px;\n",
    "        background-color: #161b22; /* Slightly lighter container for contrast */\n",
    "    }\n",
    "    h1, h2, h3 {\n",
    "        color: #58a6ff; /* Blue for headings */\n",
    "    }\n",
    "    .st-cd, .st-ce {\n",
    "        background-color: #161b22;\n",
    "        border-radius: 8px;\n",
    "        padding: 10px;\n",
    "    }\n",
    "    .st-emotion-cache-1629p8f { /* Targetting metrics container for better alignment */\n",
    "        gap: 1.5rem;\n",
    "    }\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "\n",
    "st.title(\"🌌 Holistic Horizon EPM Dashboard: Multi-Modal Integrated Prediction\")\n",
    "st.markdown(\"### Fusing Quantitative Time-Series Data with Qualitative LLM Context\")\n",
    "\n",
    "st.info(\"\"\"\n",
    "    This dashboard provides a visualization of the Enterprise Performance Management ($\\text{EPM}$) metrics used for training the Hybrid Fusion Model. The core objective is to reduce prediction error for **Emissions Intensity** by incorporating **simulated narrative context** ($\\text{LLM}$ embeddings).\n",
    "\"\"\")\n",
    "\n",
    "# --- 1. KPI Overview (Gauges and Metrics) ---\n",
    "st.markdown(\"---\")\n",
    "st.subheader(\"Key Performance Indicators (EPM Snapshot)\")\n",
    "\n",
    "if TARGET_CLEANED in df.columns:\n",
    "    avg_emissions = df[TARGET_CLEANED].mean()\n",
    "    avg_sustainability = df['Sustainability_Score'].mean()\n",
    "    avg_renewable = df['Renewable_Energy_Share'].mean()\n",
    "    avg_profit = df['Net_Profit_Margin'].mean()\n",
    "\n",
    "    col1, col2, col3, col4 = st.columns(4)\n",
    "\n",
    "    # Metric 1: Emissions (Goal: Lower)\n",
    "    col1.metric(\"Avg. Emissions Intensity\", f\"{avg_emissions:.2f} kg/MWh\", delta_color=\"inverse\")\n",
    "\n",
    "    # Metric 2: Net Profit Margin\n",
    "    col2.metric(\"Avg. Net Profit Margin\", f\"{avg_profit:.2f} %\", delta=f\"{df['Net_Profit_Margin'].std():.2f} Std Dev\")\n",
    "\n",
    "    # Metric 3: Sustainability Score (Gauge)\n",
    "    with col3:\n",
    "        fig_sustainability = go.Figure(go.Indicator(\n",
    "            mode = \"gauge+number\",\n",
    "            value = avg_sustainability,\n",
    "            title = {'text': \"Avg. Sustainability Score\", 'font': {'size': 14}},\n",
    "            gauge = {\n",
    "                'axis': {'range': [None, 100], 'tickwidth': 1, 'tickcolor': \"darkgray\"},\n",
    "                'bar': {'color': \"#58a6ff\"},\n",
    "                'steps': [\n",
    "                    {'range': [0, 60], 'color': \"red\"},\n",
    "                    {'range': [60, 80], 'color': \"yellow\"},\n",
    "                    {'range': [80, 100], 'color': \"green\"}],\n",
    "                'threshold': {'line': {'color': \"white\", 'width': 4}, 'thickness': 0.75, 'value': 85}}\n",
    "        ))\n",
    "        fig_sustainability.update_layout(height=200, margin=dict(t=50, b=0, l=10, r=10), template=\"plotly_dark\")\n",
    "        st.plotly_chart(fig_sustainability, use_container_width=True)\n",
    "\n",
    "    # Metric 4: Renewable Share (Gauge)\n",
    "    with col4:\n",
    "        fig_renewable = go.Figure(go.Indicator(\n",
    "            mode = \"gauge+number\",\n",
    "            value = avg_renewable,\n",
    "            title = {'text': \"Avg. Renewable Share\", 'font': {'size': 14}},\n",
    "            gauge = {\n",
    "                'axis': {'range': [None, 100], 'tickwidth': 1, 'tickcolor': \"darkgray\"},\n",
    "                'bar': {'color': \"#2A803B\"}, # Darker green for energy\n",
    "                'steps': [\n",
    "                    {'range': [0, 25], 'color': \"red\"},\n",
    "                    {'range': [25, 50], 'color': \"yellow\"},\n",
    "                    {'range': [50, 100], 'color': \"green\"}],\n",
    "                'threshold': {'line': {'color': \"white\", 'width': 4}, 'thickness': 0.75, 'value': 60}}\n",
    "        ))\n",
    "        fig_renewable.update_layout(height=200, margin=dict(t=50, b=0, l=10, r=10), template=\"plotly_dark\")\n",
    "        st.plotly_chart(fig_renewable, use_container_width=True)\n",
    "\n",
    "\n",
    "# --- 2. Dual-Axis Time-Series Trend ---\n",
    "st.markdown(\"---\")\n",
    "st.subheader(\"Quantitative Path Visualization: Simulated EPM Trend\")\n",
    "\n",
    "tab1, tab2 = st.tabs([\"Dual-Axis Trend (Target vs. Driver)\", \"Feature Distributions\"])\n",
    "\n",
    "with tab1:\n",
    "    fig_ts = go.Figure()\n",
    "\n",
    "    # Emissions (Primary Axis) - Orange/Red for warning/emissions\n",
    "    fig_ts.add_trace(go.Scatter(\n",
    "        x=df['Time_Step'], \n",
    "        y=df[TARGET_CLEANED], \n",
    "        mode='lines', \n",
    "        name=TARGET_COLUMN, \n",
    "        yaxis='y1',\n",
    "        line=dict(color='#ff7f0e', width=3)\n",
    "    ))\n",
    "\n",
    "    # Renewable Energy Share (Secondary Axis) - Blue/Green for progress\n",
    "    fig_ts.add_trace(go.Scatter(\n",
    "        x=df['Time_Step'], \n",
    "        y=df['Renewable_Energy_Share'], \n",
    "        mode='lines', \n",
    "        name='Renewable Energy Share (%)', \n",
    "        yaxis='y2',\n",
    "        line=dict(color='#1f77b4', dash='dash', width=2)\n",
    "    ))\n",
    "\n",
    "    fig_ts.update_layout(\n",
    "        title='Simulated EPM Trend: Emissions Intensity (Target) vs. Renewable Share (Feature)',\n",
    "        xaxis_title='Simulated Time Step (Company Index)',\n",
    "        yaxis=dict(\n",
    "            title=TARGET_COLUMN,\n",
    "            titlefont=dict(color='#ff7f0e'),\n",
    "            tickfont=dict(color='#ff7f0e'),\n",
    "            gridcolor='#161b22'\n",
    "        ),\n",
    "        yaxis2=dict(\n",
    "            title='Renewable Energy Share (%)',\n",
    "            titlefont=dict(color='#1f77b4'),\n",
    "            tickfont=dict(color='#1f77b4'),\n",
    "            overlaying='y',\n",
    "            side='right',\n",
    "            gridcolor='#161b22'\n",
    "        ),\n",
    "        height=550,\n",
    "        template=\"plotly_dark\"\n",
    "    )\n",
    "    st.plotly_chart(fig_ts, use_container_width=True)\n",
    "    st.markdown(\"\"\"\n",
    "        *Observation:* This dual-axis chart shows the sequential data fed to the $\\text{LSTM}$ model. The **Hybrid Model** uses this history *plus* the **Narrative Embedding** corresponding to the prediction step to capture non-linear market/policy impacts.\n",
    "    \"\"\")\n",
    "\n",
    "with tab2:\n",
    "    selected_feature = st.selectbox(\n",
    "        'Select a Feature to view its Distribution:',\n",
    "        options=[\n",
    "            'Revenue_USD', 'Net_Profit_Margin', 'Energy_Efficiency', \n",
    "            'Sustainability_Score', TARGET_CLEANED\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    if selected_feature in df.columns:\n",
    "        fig_dist = px.histogram(\n",
    "            df, \n",
    "            x=selected_feature, \n",
    "            title=f'Distribution of {selected_feature.replace(\"_\", \" \")}',\n",
    "            color_discrete_sequence=['#5D9C3E'],\n",
    "            template=\"plotly_dark\"\n",
    "        )\n",
    "        fig_dist.update_layout(height=450)\n",
    "        st.plotly_chart(fig_dist, use_container_width=True)\n",
    "\n",
    "# --- 3. Feature Relationship (Scatter Plot) ---\n",
    "st.markdown(\"---\")\n",
    "st.subheader(\"Feature Correlation: Sustainability vs. Emissions\")\n",
    "\n",
    "if 'Sustainability_Score' in df.columns:\n",
    "    fig_scatter = px.scatter(\n",
    "        df, \n",
    "        x='Sustainability_Score', \n",
    "        y=TARGET_CLEANED, \n",
    "        color='Net_Profit_Margin', \n",
    "        size='Revenue_USD', # Use Revenue to denote company size/impact\n",
    "        hover_data=['Company_ID'],\n",
    "        title=f'Emissions Intensity vs. Sustainability Score, Colored by Profit Margin',\n",
    "        labels={\n",
    "            TARGET_CLEANED: TARGET_COLUMN,\n",
    "            'Sustainability_Score': 'Overall Sustainability Score (0-100)',\n",
    "            'Net_Profit_Margin': 'Net Profit Margin (%)'\n",
    "        },\n",
    "        color_continuous_scale=px.colors.sequential.Viridis,\n",
    "        template='plotly_dark'\n",
    "    )\n",
    "    fig_scatter.update_layout(height=600, coloraxis_colorbar=dict(title=\"Profit Margin %\"))\n",
    "    st.plotly_chart(fig_scatter, use_container_width=True)\n",
    "    st.markdown(\"\"\"\n",
    "        *Insight:* Outliers in this plot—companies with high scores but high emissions, or low scores but high profit—are where the **Narrative Path** is most crucial. The qualitative context can explain these non-linear relationships, which a pure time-series model would struggle to capture.\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "# --- 4. Project Interpretation and Architecture ---\n",
    "st.markdown(\"---\")\n",
    "st.subheader(\"Hybrid Model Architecture: Fusing Whispers to Roars\")\n",
    "\n",
    "st.markdown(\"\"\"\n",
    "    The \"Holistic Horizon\" model is built on a **Dual-Input Fusion Architecture** to achieve superior $\\text{EPM}$ prediction:\n",
    "\n",
    "    1.  **Quantitative Path (Time-Series $\\text{LSTM}$):** Learns the temporal dependencies and patterns inherent in the numerical $\\text{KPIs}$ (e.g., historical revenue, efficiency, and emissions).\n",
    "    2.  **Qualitative Path ($\\text{Dense Network}$):** Processes the **Narrative Embedding Vector** (simulated $\\text{LLM}$ output) to capture the semantic context, such as policy shifts, strategic management decisions, or unplanned operational events.\n",
    "    3.  **Fusion:** The feature vectors from both paths are **concatenated** at a bottleneck layer, allowing the model to learn combined weights and predict the target based on **both historical trends and qualitative context**.\n",
    "\n",
    "    This integration is why the Hybrid Model is expected to outperform the pure $\\text{LSTM}$ baseline.\n",
    "\"\"\")\n",
    "\n",
    "st.code(\n",
    "    \"\"\"\n",
    "    # Conceptual Keras Fusion\n",
    "    ts_input = Input(shape=(SEQUENCE_LENGTH, num_ts_features))\n",
    "    narrative_input = Input(shape=(EMBEDDING_DIMENSION,))\n",
    "\n",
    "    ts_output = LSTM_path(ts_input)            # Quantitative feature vector (e.g., 16 units)\n",
    "    narrative_output = Dense_path(narrative_input) # Qualitative feature vector (e.g., 16 units)\n",
    "\n",
    "    fusion = Concatenate()([ts_output, narrative_output])\n",
    "    prediction = Dense(1)(fusion)\n",
    "    \"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab82664-5684-4046-9eb5-8773525ea142",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
